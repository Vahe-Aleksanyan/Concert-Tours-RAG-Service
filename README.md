To run the project, configure your .env file with your OPENAI_API_KEY and SERPAPI_KEY. Then simply run docker compose up --build. This will start all services and orchestrate them via Docker Compose, including health checks and persistent volumes. The project flow starts with 40 manually generated documents—some relevant to concert tours and others not. The ingestion service processes each document, summarizes it, and decides whether to keep it based on keyword detection. Relevant documents are split into chunks and stored in a vector database (Chroma). After that, users can ask questions that get answered strictly using the ingested content—general questions are restricted. We also built an internet search feature for live artist lookups using Serper.dev. Finally, we added a Streamlit UI for easy interaction and testing. All logic is wrapped in clear, well-documented REST APIs. Screenshots and visual demos are included for clarity.

To prepare the system for first-time use, run python scripts/synthetic_data_generator.py to create mock tour documents, then ingest them via python scripts/ingest_bulk.py. This step is only required once. All data is persisted inside the chroma_data Docker volume, so the ingested documents will remain accessible across rebuilds without re-running the script. Once running, you can access the UI at http://localhost:8501.

Note: We also generated a set of irrelevant documents for realism and testing purposes, but only the relevant documents are ingested and used by the system. The ingestion pipeline filters out anything unrelated to concert tours.